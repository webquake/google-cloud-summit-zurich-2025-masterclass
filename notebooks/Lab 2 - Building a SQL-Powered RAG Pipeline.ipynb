{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "GSQhkviL8N4D"
      },
      "id": "GSQhkviL8N4D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Remote Connection\n",
        "\n",
        "The CONNECTION resource is the secure, IAM-governed \"handshake\" between BigQuery and other Google Cloud services, most notably Vertex AI and Google Cloud Storage.\n",
        "\n",
        "When a connection is created, it is associated with a unique service account. This service account acts as a proxy for BigQuery, and it must be granted the appropriate IAM roles to interact with external services. For instance, to call a Vertex AI model, the connection's service account needs the\n",
        "Vertex AI User (roles/aiplatform.user) role.\n",
        "\n",
        "This mechanism ensures that all interactions are authenticated, authorized, and auditable, adhering to the principle of least privilege and satisfying enterprise security requirements"
      ],
      "metadata": {
        "id": "TdqsTFEH8U97"
      },
      "id": "TdqsTFEH8U97"
    },
    {
      "cell_type": "code",
      "source": [
        "!bq mk --connection --location=US \\\n",
        "    --project_id=$GOOGLE_CLOUD_PROJECT \\\n",
        "    --connection_type=CLOUD_RESOURCE masterclass"
      ],
      "metadata": {
        "id": "x-bo7lTfxNZy"
      },
      "id": "x-bo7lTfxNZy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SERVICE_ACCT = !bq show --format=prettyjson --connection $GOOGLE_CLOUD_PROJECT.us.masterclass | grep \"serviceAccountId\" | cut -d '\"' -f 4\n",
        "SERVICE_ACCT_EMAIL = SERVICE_ACCT[-1]\n",
        "print(SERVICE_ACCT_EMAIL)"
      ],
      "metadata": {
        "id": "QE8pD2ojMetF"
      },
      "id": "QE8pD2ojMetF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "!gcloud projects add-iam-policy-binding --format=none --condition=None $PROJECT_ID --member=serviceAccount:$SERVICE_ACCT_EMAIL --role=roles/aiplatform.user"
      ],
      "metadata": {
        "id": "hKy37Xj2M0Q6"
      },
      "id": "hKy37Xj2M0Q6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud projects get-iam-policy $PROJECT_ID --flatten=bindings --filter=bindings.members:serviceAccount:$SERVICE_ACCT_EMAIL --format='value(bindings.role)'"
      ],
      "metadata": {
        "id": "nWMQ0EvXbSAy"
      },
      "id": "nWMQ0EvXbSAy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart Kernel\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "khdUBeODztGy"
      },
      "id": "khdUBeODztGy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Dataset & Register Models\n",
        "\n",
        "The remote model is the primary abstraction layer that makes in-database AI possible. Using a standard CREATE MODEL statement with a REMOTE WITH CONNECTION clause, a data engineer can register an externally hosted model as a callable object within a BigQuery dataset.\n",
        "\n",
        "This remote model can point to a powerful foundation model hosted on Vertex AI, such as Google's Gemini family or partner models from Anthropic and Mistral AI, or even a custom-trained model deployed on a Vertex AI Endpoint.\n",
        "\n",
        "The significance of this abstraction cannot be overstated. It transforms what would otherwise be a complex programming task—involving setting up a development environment, using a client library, managing API keys, and handling HTTP requests—into a simple, familiar SQL function call like ML.GENERATE_TEXT or ML.PREDICT.\n",
        "\n",
        "![GenAI Workflow](https://cloud.google.com/static/bigquery/images/gen-ai-workflow.png)\n",
        "\n",
        "The data engineer operates entirely within the BigQuery environment, while the platform handles the underlying mechanics of invoking the model, passing the data, and returning the results. This is the core mechanism that \"brings the model to the data,\" eliminating the need for complex MLOps pipelines for a wide range of use cases."
      ],
      "metadata": {
        "id": "IBFQa3uR8fX1"
      },
      "id": "IBFQa3uR8fX1"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "CREATE SCHEMA IF NOT EXISTS masterclass\n",
        "OPTIONS (\n",
        "    description = 'Data Lakehouse Mastery masterclass at the Google Cloud Summit Switzerland 2025 in Zurich',\n",
        "    location = 'US');"
      ],
      "metadata": {
        "id": "Qm_TLFP3h6Zo"
      },
      "id": "Qm_TLFP3h6Zo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "\n",
        "CREATE MODEL IF NOT EXISTS masterclass.gemini_flash\n",
        "  REMOTE WITH CONNECTION `us.masterclass`\n",
        "  OPTIONS (ENDPOINT='gemini-2.0-flash');"
      ],
      "metadata": {
        "id": "CCEgxVrRweNH"
      },
      "id": "CCEgxVrRweNH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "CREATE MODEL IF NOT EXISTS `masterclass.text_multilingual_embedding`\n",
        "REMOTE WITH CONNECTION `us.masterclass`\n",
        "OPTIONS (endpoint = 'text-multilingual-embedding-002')"
      ],
      "metadata": {
        "id": "PdnzMUoVkVTp"
      },
      "id": "PdnzMUoVkVTp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "CREATE MODEL IF NOT EXISTS `masterclass.text_embedding`\n",
        "REMOTE WITH CONNECTION `us.masterclass`\n",
        "OPTIONS (endpoint = 'text-embedding-005')"
      ],
      "metadata": {
        "id": "duPZGRSBBm1j"
      },
      "id": "duPZGRSBBm1j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Semantic Representations with ML.GENERATE_EMBEDDING"
      ],
      "metadata": {
        "id": "PRis8czcCimq"
      },
      "id": "PRis8czcCimq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate and Store Embeddings\n",
        "\n",
        "Next, you will execute a CREATE TABLE AS SELECT statement. This query uses the ML.GENERATE_EMBEDDING function to process the abstract text from the patents table. The function outputs a 768-dimension vector for each abstract, which is then stored in a new table alongside the original patent ID and title for future reference."
      ],
      "metadata": {
        "id": "T5ufGi0wFY5U"
      },
      "id": "T5ufGi0wFY5U"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "CREATE OR REPLACE TABLE `masterclass.patent_embeddings`\n",
        "AS\n",
        "SELECT\n",
        "  publication_number,\n",
        "  title,\n",
        "  content AS abstract,\n",
        "  ml_generate_embedding_result AS embedding\n",
        "FROM\n",
        "  ML.GENERATE_EMBEDDING(\n",
        "    MODEL `masterclass.text_embedding`,\n",
        "    (\n",
        "      SELECT\n",
        "        publication_number,\n",
        "        title,\n",
        "        abstract AS content\n",
        "      FROM\n",
        "        `patents-public-data.google_patents_research.publications`\n",
        "      WHERE\n",
        "        abstract IS NOT NULL AND LENGTH(abstract) > 0\n",
        "      LIMIT 10000 -- Limit for workshop purposes\n",
        "    )\n",
        "  );"
      ],
      "metadata": {
        "id": "T6iVmMMCDcOH"
      },
      "id": "T6iVmMMCDcOH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Vector Index\n",
        "You will run the CREATE VECTOR INDEX command on the embedding column of the newly created table. This command initiates an asynchronous job to build an index using an Approximate Nearest Neighbor (ANN) algorithm. The IVF (Inverted File) index type is a common choice, which works by clustering the vectors. The DISTANCE_TYPE is set to COSINE, which is effective for measuring the similarity of text embeddings."
      ],
      "metadata": {
        "id": "VeoNdqrXFFnd"
      },
      "id": "VeoNdqrXFFnd"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "\n",
        "CREATE OR REPLACE VECTOR INDEX `patent_embedding_index`\n",
        "ON `masterclass.patent_embeddings`(embedding)\n",
        "OPTIONS(index_type = 'IVF', distance_type = 'COSINE');"
      ],
      "metadata": {
        "id": "YDyoE-j3Enm_"
      },
      "id": "YDyoE-j3Enm_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerating Search with VECTOR_SEARCH\n",
        "\n",
        "With embeddings created, the next step is to enable efficient searching. A brute-force search comparing a query vector to millions of document vectors would be computationally expensive. Vector indexes solve this problem.\n"
      ],
      "metadata": {
        "id": "Iz_L2L_6E4li"
      },
      "id": "Iz_L2L_6E4li"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform Semantic Search\n",
        "Now, you can perform a semantic search using the VECTOR_SEARCH function. This query first generates an embedding for a user's natural language question (e.g., \"inventions related to solar panel efficiency\"). It then uses this query vector to search the indexed patent table, efficiently finding the top_k most semantically similar patent abstracts based on cosine similarity."
      ],
      "metadata": {
        "id": "2fWkTSBuF-5U"
      },
      "id": "2fWkTSBuF-5U"
    },
    {
      "cell_type": "code",
      "source": [
        "SEARCH_QUERY = \"inventions related to solar panel efficiency\"  # @param {type:\"string\"}\n",
        "BQ_PARAMS = {\"search_query\": SEARCH_QUERY}\n",
        "\n",
        "print(f\"BQ_PARAMS: {BQ_PARAMS}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SNQqPpOHGuBx"
      },
      "id": "SNQqPpOHGuBx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --params $BQ_PARAMS\n",
        "SELECT\n",
        "  base.publication_number,\n",
        "  base.title,\n",
        "  base.abstract,\n",
        "  distance\n",
        "FROM\n",
        "  VECTOR_SEARCH(\n",
        "    TABLE `masterclass.patent_embeddings`,\n",
        "    'embedding',\n",
        "    (\n",
        "      SELECT ml_generate_embedding_result AS embedding\n",
        "      FROM ML.GENERATE_EMBEDDING(\n",
        "        MODEL `masterclass.text_embedding`,\n",
        "        (SELECT @search_query AS content)\n",
        "      )\n",
        "    ),\n",
        "    top_k => 5\n",
        "  );\n"
      ],
      "metadata": {
        "id": "UYKsD-_TJtUp"
      },
      "id": "UYKsD-_TJtUp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct the RAG Query\n",
        "The final task is to construct a single, multi-step SQL query using Common Table Expressions (CTEs). This query orchestrates the entire RAG workflow:\n",
        "\n",
        "1. A CTE (retrieved_context) uses VECTOR_SEARCH to find the top 5 relevant patent abstracts for a given question, just as in the previous step.\n",
        "2. A second CTE (prompt_construction) concatenates the user's question with the retrieved abstracts to form a single, comprehensive prompt.\n",
        "3. The final SELECT statement passes this dynamically constructed prompt to the Gemini Flash model via ML.GENERATE_TEXT. The prompt explicitly instructs the model to answer the question based only on the provided context, which helps to ground the response in factual data and reduce hallucinations.\n"
      ],
      "metadata": {
        "id": "js5xVBxlKvyb"
      },
      "id": "js5xVBxlKvyb"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --params $BQ_PARAMS\n",
        "  SELECT\n",
        "    base.title,\n",
        "    base.abstract\n",
        "  FROM\n",
        "    VECTOR_SEARCH(\n",
        "      TABLE `masterclass.patent_embeddings`,\n",
        "      'embedding',\n",
        "      (\n",
        "        SELECT ml_generate_embedding_result AS embedding\n",
        "        FROM ML.GENERATE_EMBEDDING(\n",
        "          MODEL `masterclass.text_embedding`,\n",
        "          (SELECT @search_query AS content)\n",
        "        )\n",
        "      ),\n",
        "      top_k => 5\n",
        "    )"
      ],
      "metadata": {
        "id": "hdlcqfgnLfJZ"
      },
      "id": "hdlcqfgnLfJZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery rag_result --params $BQ_PARAMS\n",
        "WITH retrieved_context AS (\n",
        "  SELECT\n",
        "    base.title,\n",
        "    base.abstract\n",
        "  FROM\n",
        "    VECTOR_SEARCH(\n",
        "      TABLE `masterclass.patent_embeddings`,\n",
        "      'embedding',\n",
        "      (\n",
        "        SELECT ml_generate_embedding_result AS embedding\n",
        "        FROM ML.GENERATE_EMBEDDING(\n",
        "          MODEL `masterclass.text_embedding`,\n",
        "          (SELECT @search_query AS content)\n",
        "        )\n",
        "      ),\n",
        "      top_k => 5\n",
        "    )\n",
        "),\n",
        "prompt_construction AS (\n",
        "  SELECT\n",
        "    CONCAT(\n",
        "      \"\"\"Question: What are some inventions related to solar panel efficiency?\n",
        "      Answer the question based ONLY on the following context.\n",
        "      Context: \"\"\",\n",
        "      STRING_AGG(retrieved_context.abstract, ' ')\n",
        "    ) AS prompt\n",
        "  FROM retrieved_context\n",
        ")\n",
        "SELECT\n",
        "  ml_generate_text_result['candidates'][0]['content']['parts'][0]['text'] AS generated_answer\n",
        "FROM\n",
        "  ML.GENERATE_TEXT(\n",
        "    MODEL `masterclass.gemini_flash`,\n",
        "    TABLE prompt_construction\n",
        "  );"
      ],
      "metadata": {
        "id": "N1-u8NMQLI0C"
      },
      "id": "N1-u8NMQLI0C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown as md\n",
        "md(rag_result['generated_answer'][0].replace(\"\\\"\",\"\").replace(\"\\\\n\",\"\\n\"))"
      ],
      "metadata": {
        "id": "XrGdG034OilU"
      },
      "id": "XrGdG034OilU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning up"
      ],
      "metadata": {
        "id": "aZkWZ3kJnqUH"
      },
      "id": "aZkWZ3kJnqUH"
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# !bq rm -r -f $PROJECT_ID:masterclass\n",
        "# !bq rm --connection --project_id=$PROJECT_ID --location=us masterclass\n",
        "#"
      ],
      "metadata": {
        "id": "YAZNhcWBns5m"
      },
      "id": "YAZNhcWBns5m",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "masterclass/Lab 2 - Building a SQL-Powered RAG Pipeline.ipynb",
      "collapsed_sections": [
        "TdqsTFEH8U97",
        "aZkWZ3kJnqUH"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
